{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64733,"databundleVersionId":8171035,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GeoLifeClef - TechieDuo Submission\n\nPredicting the plant species present at a given location is helpful for many biodiversity management and conservation scenarios. Our approach towards the solution is by first merging all the training datasets based on their survey ID, then preprocessing the data and analyzing it further to get a better understanding. A similar approach is used on the test dataset as well. This helps to bring out the similarity between the training and the test data and further ease out its usage in the model for prediction.\n\nTeam Members:\n1. Vijay Srinivas K\n2. Vidisha Desai\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-10T14:12:17.957968Z","iopub.execute_input":"2024-05-10T14:12:17.958359Z","iopub.status.idle":"2024-05-10T14:12:17.963784Z","shell.execute_reply.started":"2024-05-10T14:12:17.958323Z","shell.execute_reply":"2024-05-10T14:12:17.962424Z"}}},{"cell_type":"code","source":"# all import statements\nimport numpy as np\nimport pandas as pd\nimport os ","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:12:07.548365Z","iopub.execute_input":"2024-05-25T02:12:07.548854Z","iopub.status.idle":"2024-05-25T02:12:07.554875Z","shell.execute_reply.started":"2024-05-25T02:12:07.548815Z","shell.execute_reply":"2024-05-25T02:12:07.553437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing:\n\n*The dataset datas are analysed and all the files in it are found. Later the required fields are found out from these files and is made into a single large file containing all the data matched based on the survey ID  in a single file. The required data columns and labels are found from this combined dataset*\n\n","metadata":{}},{"cell_type":"code","source":"# getting all the files to process\ncsv_files = []    \nfor root, dirs, files in os.walk(\"/kaggle/input/geolifeclef-2024\"):\n    for file in files:\n        if file.endswith(\".csv\"):\n            csv_files.append(os.path.join(root, file))\n#print(csv_files)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:12:07.557280Z","iopub.execute_input":"2024-05-25T02:12:07.557808Z","iopub.status.idle":"2024-05-25T02:16:10.660838Z","shell.execute_reply.started":"2024-05-25T02:12:07.557769Z","shell.execute_reply":"2024-05-25T02:16:10.659799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Viewing the main dataset features","metadata":{}},{"cell_type":"code","source":"# Useful functions\ndef display_file_info(file_list):\n    for fpath in file_list:\n        df = pd.read_csv(fpath)\n        print(fpath.split('/')[-1], \":\", df.shape)\n        print(df.columns)\n        \ndef get_files(csv_files, con_str):\n    op_files = []\n    for fpath in csv_files:\n        if not con_str in fpath:\n            op_files.append(fpath)\n    return op_files","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:16:10.662747Z","iopub.execute_input":"2024-05-25T02:16:10.663137Z","iopub.status.idle":"2024-05-25T02:16:10.670584Z","shell.execute_reply.started":"2024-05-25T02:16:10.663103Z","shell.execute_reply":"2024-05-25T02:16:10.669244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting only the train data\ncsv_files_train = get_files(csv_files, \"test\")\nnon_time_series_train = get_files(csv_files_train, \"time_series\")\n\n#display_file_info(csv_files)\n#print(\"\\nNon Times series data:\")\ndisplay_file_info(non_time_series_train)\nprint(\"\\nData sets to train: \", len(non_time_series_train))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:16:10.672484Z","iopub.execute_input":"2024-05-25T02:16:10.672929Z","iopub.status.idle":"2024-05-25T02:16:41.163156Z","shell.execute_reply.started":"2024-05-25T02:16:10.672869Z","shell.execute_reply":"2024-05-25T02:16:41.161973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now, we create a combined df of all the training datasets","metadata":{}},{"cell_type":"code","source":"# train data\nnon_time_series_train_1 = non_time_series_train\nprint(non_time_series_train_1)\nnon_time_series_train_1.remove(\"/kaggle/input/geolifeclef-2024/GLC24_P0_metadata_train.csv\")\nnon_time_series_train_1.remove(\"/kaggle/input/geolifeclef-2024/GLC24_SAMPLE_SUBMISSION.csv\")\nnon_time_series_train_1.remove(\"/kaggle/input/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-train-bioclimatic_monthly.csv\")\n    \nnts_train_dfs = []\nfor fpath in non_time_series_train_1:\n    df = pd.read_csv(fpath)\n    print(fpath.split('/')[-1], \":\", df.shape)\n    nts_train_dfs.append(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:16:41.167139Z","iopub.execute_input":"2024-05-25T02:16:41.169457Z","iopub.status.idle":"2024-05-25T02:16:43.500429Z","shell.execute_reply.started":"2024-05-25T02:16:41.169420Z","shell.execute_reply":"2024-05-25T02:16:43.499167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merging and creating a combined df\nmerge_key = \"surveyId\"\nmerged_train_df = nts_train_dfs[0]\nfor i in range(1, len(nts_train_dfs)):\n    merged_train_df = merged_train_df.merge(nts_train_dfs[i], on = merge_key)\nprint(merged_train_df.head(5))\nmerged_train_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:16:43.502073Z","iopub.execute_input":"2024-05-25T02:16:43.502405Z","iopub.status.idle":"2024-05-25T02:16:45.451351Z","shell.execute_reply.started":"2024-05-25T02:16:43.502376Z","shell.execute_reply":"2024-05-25T02:16:45.450013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = list(merged_train_df.columns)\nfor i in val:\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:16:45.453078Z","iopub.execute_input":"2024-05-25T02:16:45.453475Z","iopub.status.idle":"2024-05-25T02:16:45.460396Z","shell.execute_reply.started":"2024-05-25T02:16:45.453444Z","shell.execute_reply":"2024-05-25T02:16:45.458935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning\n\nWe clean the data to get rid of duplicate rows, null values attributes and make the data uniformly distributed.","metadata":{}},{"cell_type":"code","source":"# we have the merged_train_df now \nmerged_train_df = merged_train_df.dropna(axis = 0)\nprint(merged_train_df.shape)\nmerged_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:16:45.462468Z","iopub.execute_input":"2024-05-25T02:16:45.463274Z","iopub.status.idle":"2024-05-25T02:16:46.168651Z","shell.execute_reply.started":"2024-05-25T02:16:45.463232Z","shell.execute_reply":"2024-05-25T02:16:46.167304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# getting rid of unwanted multiple instances of rows\ncondition = (merged_train_df[\"speciesId_x\"] == merged_train_df[\"speciesId_y\"]) # removing all the data mismatches\nmerged_train_df_clean = merged_train_df[condition]\nmerged_train_df_clean = merged_train_df_clean.drop(axis = 1, columns = [\"speciesId_y\"])\nmerged_train_df_clean = merged_train_df_clean.rename(columns={'speciesId_x': 'speciesId'})\nprint(merged_train_df_clean.shape)\nmerged_train_df_clean.head()'''","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:16:46.170862Z","iopub.execute_input":"2024-05-25T02:16:46.171319Z","iopub.status.idle":"2024-05-25T02:16:46.180285Z","shell.execute_reply.started":"2024-05-25T02:16:46.171279Z","shell.execute_reply":"2024-05-25T02:16:46.179016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Note : Now we have a clean DataFrame merged_train_df_clean which has proper data and has been cleansed off all the unwated, repetitve data that might have been present in it. \nNow, we use this data to train a model, and test it's accuracy ","metadata":{}},{"cell_type":"markdown","source":"## Test Data Creation:\nIn this section, we follow the same process as above to create the test data. for using it to test the model that we will be training later on.","metadata":{}},{"cell_type":"code","source":"def create_test_data(csv_files):\n    # getting the test only\n    csv_files_test = get_files(csv_files, \"train\")\n    non_time_series_test = get_files(csv_files_test, \"time_series\")\n    \n    non_time_series_test_1 = non_time_series_test\n    print(non_time_series_test_1)\n    \n    non_time_series_test_1.remove('/kaggle/input/geolifeclef-2024/GLC24_SAMPLE_SUBMISSION.csv')\n    non_time_series_test_1.remove('/kaggle/input/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-test-bioclimatic_monthly.csv')\n    \n    nts_test_dfs = []\n    for fpath in non_time_series_test_1:\n        df = pd.read_csv(fpath)\n        print(fpath.split('/')[-1], \":\", df.shape)\n        nts_test_dfs.append(df)\n        \n    merge_key = \"surveyId\"\n    merged_test_df = nts_test_dfs[0]\n    for i in range(1, len(nts_test_dfs)):\n        merged_test_df = merged_test_df.merge(nts_test_dfs[i], on = merge_key)\n    print(merged_test_df.head(5))\n    merged_test_df.shape\n    \n    merged_test_df.fillna(0, inplace=True)\n    print(merged_test_df.shape)\n    merged_test_df.head()\n    \n    return merged_test_df\n    \nmerged_test_df = create_test_data(csv_files)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:18:30.381585Z","iopub.execute_input":"2024-05-25T02:18:30.383368Z","iopub.status.idle":"2024-05-25T02:18:30.492215Z","shell.execute_reply.started":"2024-05-25T02:18:30.383309Z","shell.execute_reply":"2024-05-25T02:18:30.490288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Converting categorical data to numerical data to avoid any issues later in the model training. The changes are made in training and test data. The categorical values of country and region have been mapped to numerical data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\n# for training data\nmerged_train_df['country_encoded'] = label_encoder.fit_transform(merged_train_df['country'])\nmerged_train_df=merged_train_df.drop(axis=1,columns = [\"country\"])\n\nmerged_train_df['region_encoded'] = label_encoder.fit_transform(merged_train_df['region'])\nmerged_train_df=merged_train_df.drop(axis=1,columns = [\"region\"])\n\nprint(merged_train_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:18:37.732408Z","iopub.execute_input":"2024-05-25T02:18:37.732899Z","iopub.status.idle":"2024-05-25T02:18:38.964433Z","shell.execute_reply.started":"2024-05-25T02:18:37.732848Z","shell.execute_reply":"2024-05-25T02:18:38.963145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\n# for test data\nmerged_test_df['country_encoded'] = label_encoder.fit_transform(merged_test_df['country'])\nmerged_test_df=merged_test_df.drop(axis=1,columns = [\"country\"])\n\nmerged_test_df['region_encoded'] = label_encoder.fit_transform(merged_test_df['region'])\nmerged_test_df=merged_test_df.drop(axis=1,columns = [\"region\"])\n\nprint(merged_test_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:18:48.204409Z","iopub.execute_input":"2024-05-25T02:18:48.204896Z","iopub.status.idle":"2024-05-25T02:18:48.243546Z","shell.execute_reply.started":"2024-05-25T02:18:48.204848Z","shell.execute_reply":"2024-05-25T02:18:48.242165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model:\n\n>KNN Classfier Model:\nWe have identified that this is a classification problem and have decided to go ahead with KNN Classfier Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\ny_train = merged_train_df['speciesId']\nX_train = merged_train_df.drop(columns='speciesId')\n\nX_test = merged_test_df\n\nX_train = X_train.replace([np.inf, -np.inf], np.nan)\nX_train = X_train.dropna()\n\ny_train = y_train.loc[X_train.index]\n\nprint(X_test.shape)\nX_test = X_test.replace([np.inf, -np.inf], np.nan)\nprint(X_test.shape)\nX_train = X_train[~X_train.isin([np.nan, np.inf, -np.inf]).any(axis=1)]\n# Replace inf and -inf values with 0\nX_test.replace([np.inf, -np.inf], 0, inplace=True)\nprint(X_test.shape)\n# Ensure X_test has the same columns as X_train, filling any missing columns with 0\nX_test = X_test.reindex(columns=X_train.columns, fill_value=0)\nprint(X_test.shape)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Train the classifier\nknn.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:19:30.840389Z","iopub.execute_input":"2024-05-25T02:19:30.840944Z","iopub.status.idle":"2024-05-25T02:19:41.456975Z","shell.execute_reply.started":"2024-05-25T02:19:30.840869Z","shell.execute_reply":"2024-05-25T02:19:41.456052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.fillna(0, inplace=True)\ny_pred = knn.predict(X_test)\n\npredictions = pd.DataFrame(y_pred, columns=['predictions'])\npredictions['surveyId'] = X_test['surveyId'].values\n\ncolumn_order = ['surveyId','predictions']\n\npredictions = predictions[column_order]\n\npredictions.to_csv('submission.csv', index=False)\n\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T02:19:44.052458Z","iopub.execute_input":"2024-05-25T02:19:44.053918Z","iopub.status.idle":"2024-05-25T02:20:03.437236Z","shell.execute_reply.started":"2024-05-25T02:19:44.053836Z","shell.execute_reply":"2024-05-25T02:20:03.436310Z"},"trusted":true},"execution_count":null,"outputs":[]}]}